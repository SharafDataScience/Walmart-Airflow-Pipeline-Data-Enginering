{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f321781-585b-499f-aa34-67008282f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG for ETL workflow with tests\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Default arguments\n",
    "default_args = {\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'sla': timedelta(hours=2)\n",
    "}\n",
    "\n",
    "# Define DAG\n",
    "with DAG(\n",
    "    dag_id='etl_workflow_dag',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=None,  # You can put a cron schedule if needed\n",
    "    catchup=False\n",
    ") as dag:\n",
    "\n",
    "    ######################\n",
    "    # Functions\n",
    "    ######################\n",
    "    \n",
    "    def extract(**kwargs):\n",
    "        store_data_path = kwargs['params']['store_data_path']\n",
    "        extra_data_path = kwargs['params']['extra_data_path']\n",
    "        \n",
    "        store_data = pd.read_csv(store_data_path)\n",
    "        extra_data = pd.read_parquet(extra_data_path)\n",
    "        merged_df = store_data.merge(extra_data, on=\"index\")\n",
    "        \n",
    "        merged_df.to_csv('/tmp/merged_data.csv', index=False)\n",
    "    \n",
    "    def transform(**kwargs):\n",
    "        merged_df = pd.read_csv('/tmp/merged_data.csv')\n",
    "        \n",
    "        merged_df.fillna({\n",
    "            'CPI': merged_df['CPI'].mean(),\n",
    "            'Weekly_Sales': merged_df['Weekly_Sales'].mean(),\n",
    "            'Unemployment': merged_df['Unemployment'].mean(),\n",
    "        }, inplace=True)\n",
    "        \n",
    "        merged_df[\"Date\"] = pd.to_datetime(merged_df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "        merged_df[\"Month\"] = merged_df[\"Date\"].dt.month\n",
    "        \n",
    "        merged_df = merged_df.loc[merged_df[\"Weekly_Sales\"] > 10000, :]\n",
    "        \n",
    "        merged_df = merged_df.drop([\n",
    "            \"index\", \"Temperature\", \"Fuel_Price\",\n",
    "            \"MarkDown1\", \"MarkDown2\", \"MarkDown3\",\n",
    "            \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"\n",
    "        ], axis=1)\n",
    "        \n",
    "        merged_df.to_csv('/tmp/clean_data.csv', index=False)\n",
    "    \n",
    "    def aggregate(**kwargs):\n",
    "        clean_df = pd.read_csv('/tmp/clean_data.csv')\n",
    "        \n",
    "        holidays_sales = clean_df[[\"Month\", \"Weekly_Sales\"]]\n",
    "        agg_data = (holidays_sales.groupby(\"Month\")\n",
    "                    .agg(Avg_Sales=(\"Weekly_Sales\", \"mean\"))\n",
    "                    .reset_index()\n",
    "                    .round(2))\n",
    "        \n",
    "        agg_data.to_csv('/tmp/agg_data.csv', index=False)\n",
    "    \n",
    "    def load(**kwargs):\n",
    "        os.makedirs('/home/airflow/output', exist_ok=True)\n",
    "        \n",
    "        os.rename('/tmp/clean_data.csv', '/home/airflow/output/clean_data.csv')\n",
    "        os.rename('/tmp/agg_data.csv', '/home/airflow/output/agg_data.csv')\n",
    "\n",
    "    def validate(**kwargs):\n",
    "        paths = [\n",
    "            '/home/airflow/output/clean_data.csv',\n",
    "            '/home/airflow/output/agg_data.csv'\n",
    "        ]\n",
    "        for path in paths:\n",
    "            if not os.path.exists(path):\n",
    "                raise Exception(f\"Validation Failed! File not found: {path}\")\n",
    "\n",
    "    ######################\n",
    "    # Tasks\n",
    "    ######################\n",
    "\n",
    "    start = EmptyOperator(task_id=\"start\")\n",
    "    \n",
    "    wait_for_file = FileSensor(\n",
    "        task_id='wait_for_store_data',\n",
    "        filepath='/home/airflow/input/grocery_sales.csv',\n",
    "        poke_interval=30,\n",
    "        timeout=600,\n",
    "    )\n",
    "\n",
    "    extract_task = PythonOperator(\n",
    "        task_id='extract_task',\n",
    "        python_callable=extract,\n",
    "        provide_context=True,\n",
    "        params={\n",
    "            'store_data_path': '/home/airflow/input/grocery_sales.csv',\n",
    "            'extra_data_path': '/home/airflow/input/extra_data.parquet'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    transform_task = PythonOperator(\n",
    "        task_id='transform_task',\n",
    "        python_callable=transform,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    aggregate_task = PythonOperator(\n",
    "        task_id='aggregate_task',\n",
    "        python_callable=aggregate,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    load_task = PythonOperator(\n",
    "        task_id='load_task',\n",
    "        python_callable=load,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    validation_task = PythonOperator(\n",
    "        task_id='validation_task',\n",
    "        python_callable=validate,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    end = EmptyOperator(task_id=\"end\")\n",
    "\n",
    "    ######################\n",
    "    # Task Pipeline\n",
    "    ######################\n",
    "\n",
    "    start >> wait_for_file >> extract_task >> transform_task >> aggregate_task >> load_task >> validation_task >> end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
